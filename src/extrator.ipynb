{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio  # executar as requisições de forma asíncrona\n",
    "from typing import List\n",
    "from typing import Any\n",
    "\n",
    "import bs4\n",
    "import httpx\n",
    "import tqdm.notebook  # barra de progresso\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode # remover acentos\n",
    "from tqdm.asyncio import tqdm_asyncio  # barra de progresso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe erro customizada\n",
    "# Estou usando essa classe para debugar o código\n",
    "# toda vez que surge um erro\n",
    "class CustomError(Exception):\n",
    "    def __init__(self, message: str, *args: object, obj: Any = None) -> None:\n",
    "        self.message = message\n",
    "        self.object = obj\n",
    "        super().__init__(*args)\n",
    "    def __str__(self) -> str:\n",
    "        message  = f\"{self.message}\"\n",
    "        if self.object:\n",
    "            message += f\"; Object: {self.object}\"\n",
    "        return message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versão atualizada\n",
    "\n",
    "Esta versão coleta as páginas em porções (*chunks*).\n",
    "\n",
    "A cada porção coletada, os dados são extraídos e armazenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_links(client, links):\n",
    "    \"\"\"Requere as páginas e retorna as respostas\"\"\"\n",
    "    tasks = []\n",
    "    responses = []\n",
    "\n",
    "    for link in links:\n",
    "        tasks.append(asyncio.create_task(client.get(link)))\n",
    "\n",
    "    # Aguardar todas as tarefas acabarem\n",
    "    await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "    responses = [task.result() for task in tasks]\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de extração\n",
    "\n",
    "As funções da célula a seguir são responsáveis por extraís os dados de cada página requisitada.\n",
    "\n",
    "Uma função para cada seção da página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cada função desta célula é especializada em coletar\n",
    "# dados de porções diferentes das páginas\n",
    "\n",
    "# NOTE: apenas tabela_1 retorna DataFrame.\n",
    "# As outras retornam uma lista de DataFrame\n",
    "\n",
    "def tabela_1(tabela: bs4.element.ResultSet) -> pd.DataFrame:\n",
    "    \"\"\"Coleta dados do elemento de id=\"tabs-1\" \"\"\"\n",
    "\n",
    "    columns = []\n",
    "    data = []\n",
    "    for table_row in tabela[0].select('table')[0].select('tr'):\n",
    "        try:\n",
    "            table_data = table_row.select('td')\n",
    "            if table_data[0].has_attr('bgcolor'):\n",
    "                # remover acentos e por em caixa alta\n",
    "                column_name = unidecode(table_data[0].get_text(strip=True).upper())\n",
    "                # remover espacos e dois pontos:\n",
    "                column_name = column_name.replace(\" \",\"_\").replace(\":\", \"\")\n",
    "                # usamos get_text(strip=True) para evitar hex encoding (tipo \\xa0)\n",
    "                columns.append(column_name)\n",
    "                data.append(table_data[1].get_text(strip=True))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return pd.DataFrame([data], columns=columns)\n",
    "\n",
    "def tabela_2(tabela: bs4.element.ResultSet) -> List[pd.DataFrame]:\n",
    "    \"\"\"Coleta dados do elemento de id=\"tabs-2\" \"\"\"\n",
    "    \n",
    "    dataframes_list = []\n",
    "    \n",
    "    sub_tabelas = tabela[1].select('td')[1].select(\"table\")\n",
    "    for sub_tabela in sub_tabelas:\n",
    "        rows = sub_tabela.select('tr')\n",
    "        if \"Entrada d'água:\" in rows[0].text:\n",
    "            table_data = rows[1].select('td')\n",
    "            columns = [table_data[0].get_text(strip=True)]\n",
    "            data = [table_data[1].get_text(strip=True)]\n",
    "            try:\n",
    "                dataframes_list.append(pd.DataFrame([data], columns=columns))\n",
    "            except (AssertionError, ValueError):\n",
    "                continue\n",
    "        else:\n",
    "            if len(rows) > 1:\n",
    "                table_data = rows[0].select('td')\n",
    "                if table_data:\n",
    "                    suffix = table_data[0].get_text(strip=True)\n",
    "                    columns = []\n",
    "                    data = []\n",
    "                    if len(rows) == 2:\n",
    "                        for td in rows[1].select('td'):\n",
    "                            if ':' in td.get_text(strip=True):\n",
    "                                name, value = td.get_text(strip=True).split(':')\n",
    "                                if name:\n",
    "                                    column_name = unidecode(f\"{suffix}_{name}\".upper())\n",
    "                                    column_name = column_name.replace(\" \",\"_\").replace(\":\", \"\")\n",
    "                                    columns.append(column_name)\n",
    "                                    data.append(value)\n",
    "                    elif len(rows) == 3:\n",
    "                        for table_data_columns, table_data_rows in zip(rows[1], rows[2]):\n",
    "                            name = table_data_columns.get_text(strip=True)\n",
    "                            value = table_data_rows.get_text(strip=True)\n",
    "                            if name != '':\n",
    "                                column_name = unidecode(f\"{suffix}_{name}\".upper())\n",
    "                                column_name = column_name.replace(\" \",\"_\").replace(\":\", \"\")\n",
    "                                columns.append(column_name)\n",
    "                                data.append(value)\n",
    "                    dataframes_list.append(pd.DataFrame([data], columns=columns))\n",
    "    return dataframes_list\n",
    "\n",
    "def tabela_3(tabela: bs4.element.ResultSet) -> List[pd.DataFrame]:\n",
    "    \"\"\"Coleta dados do elemento de id=\"tabs-3\" \"\"\"\n",
    "    \n",
    "    dataframes_list = []\n",
    "    # os dados estão apenas no segundo element td\n",
    "    table_data = tabela[2].select('tr')[0].select('td')[1]\n",
    "    columns_elements = table_data.select('font')\n",
    "    table_elements = table_data.select('table')\n",
    "    for col_elem, table_elem in zip(columns_elements, table_elements):\n",
    "        suffix = col_elem.get_text(strip=True)\n",
    "        tr = table_elem.select('tr')\n",
    "        if len(tr) == 1:\n",
    "            td = tr[0].select('td')\n",
    "            name = f\"{suffix}_{td[0].get_text(strip=True)}\"\n",
    "            column = unidecode(name).replace(' ', '_').replace(':', '').upper()\n",
    "            value = td[1].get_text(strip=True)\n",
    "            dataframes_list.append(pd.DataFrame([value], columns=(column,)))\n",
    "        else:\n",
    "            names = tr[0].select('td')\n",
    "            values = tr[1].select('td')\n",
    "            columns = []\n",
    "            data = []\n",
    "            for name in names:\n",
    "                name = f\"{suffix}_{name.get_text(strip=True)}\"\n",
    "                name = name.replace(' ', '_').replace(':', '').upper()\n",
    "                columns.append(unidecode(name))\n",
    "            for value in values:\n",
    "                value = f\"{suffix}_{value.get_text(strip=True)}\"\n",
    "                data.append(value)\n",
    "            dataframes_list.append(pd.DataFrame([data], columns=columns))\n",
    "    return dataframes_list\n",
    "\n",
    "def tabela_4(tabela: bs4.element.ResultSet) -> List[pd.DataFrame]:\n",
    "    \"\"\"Coleta dados do elemento de id=\"tabs-4\" \"\"\"\n",
    "    \n",
    "    dataframes_list = []\n",
    "    # os dados estão apenas no segundo element td\n",
    "    table_data = tabela[3].select('tr')[0].select('td')[1]\n",
    "    columns_elements = table_data.select('font')\n",
    "    table_elements = table_data.select('table')\n",
    "\n",
    "    suffix = columns_elements[0].get_text(strip=True)\n",
    "\n",
    "    # tem um dado em células deslocadas (Aquífero)\n",
    "    offset_row = table_elements[0].select('tr')\n",
    "\n",
    "    # célula da esquerda\n",
    "    offset_column = offset_row[0].select('td')[0]\n",
    "    column, data = offset_column.get_text(strip=True).split(':')\n",
    "    data = data.strip()\n",
    "    column = unidecode(f\"{suffix}_{column}\").replace(' ', '').upper()\n",
    "    dataframes_list.append(pd.DataFrame([data], columns=(column,)))\n",
    "\n",
    "    # a segunda célula desloada (à direita de Aquífero)\n",
    "    columns = []\n",
    "    data = []\n",
    "    for row in offset_row[0].select('td')[1].select('tr'):\n",
    "        column_element, data_element = row.select('td')\n",
    "        \n",
    "        column = f\"{suffix}_{column_element.get_text(strip=True)}\"\n",
    "        column = unidecode(column).upper()\n",
    "        columns.append(column)\n",
    "        data.append(data_element.get_text(strip=True))\n",
    "\n",
    "    dataframes_list.append(pd.DataFrame([data], columns=columns))\n",
    "\n",
    "    # tabela abaixo de Aquífero\n",
    "    table_rows = table_elements[1].select('tr')\n",
    "    suffix = columns_elements[1].get_text(strip=True)\n",
    "    columns = []\n",
    "    data = []\n",
    "    for table_row in table_rows:\n",
    "        try:\n",
    "            column, value = table_row.select('td')\n",
    "        except ValueError as err:\n",
    "            if len(table_row.select('td')) == 3:\n",
    "                raise CustomError('pular') from err\n",
    "            raise ValueError from err\n",
    "        column = f\"{suffix}_{column.get_text(strip=True)}\"\n",
    "        column = column.replace(' ', '_').replace(':', '').upper()\n",
    "        columns.append(column)\n",
    "        data.append(value.get_text(strip=True))\n",
    "\n",
    "    dataframes_list.append(pd.DataFrame([data], columns=columns))\n",
    "\n",
    "    return dataframes_list\n",
    "\n",
    "def tabela_5(tabela: bs4.element.ResultSet) -> List[pd.DataFrame]:\n",
    "    \"\"\"Coleta dados do elemento de id=\"tabs-5\" \"\"\"\n",
    "    \n",
    "    dataframes_list = []\n",
    "    \n",
    "    # os dados estão apenas no segundo element td\n",
    "    table_data = tabela[4].select('tr')[0].select('td')[1]\n",
    "    suffix_element = table_data.select('font')\n",
    "    table_elements = table_data.select('table')\n",
    "    suffix = suffix_element[0].get_text(strip=True)\n",
    "\n",
    "    table_rows = table_elements[0].select('tr')\n",
    "    columns = []\n",
    "    data = []\n",
    "    # aqui acabei percebendo um jeito mais elegante de coletar\n",
    "    # os valores nos elementos tr. Talvez dê pra o caso\n",
    "    # das tabelas anteriores\n",
    "    for table_row in table_rows[::2]:\n",
    "        columns.extend([a.get_text(strip=True) for a in table_row.select('td')])\n",
    "    for table_row in table_rows[1::2]:\n",
    "        data.extend([a.get_text(strip=True) for a in table_row.select('td')])\n",
    "\n",
    "    for idx, (col, value) in enumerate(zip(columns, data)):\n",
    "        if col != '' and value != '':\n",
    "            columns[idx] = f\"{suffix}_{col}\".replace(' ', '_').replace(':', '')\n",
    "            columns[idx] = unidecode(columns[idx]).upper()\n",
    "\n",
    "    dataframes_list.append(pd.DataFrame([data], columns=columns))\n",
    "    \n",
    "    return dataframes_list\n",
    "\n",
    "def tabela_6(tabela: bs4.element.ResultSet) -> List[pd.DataFrame]:\n",
    "    \"\"\"Coleta dados do elemento de id=\"tabs-6\" \"\"\"\n",
    "\n",
    "    dataframes_list = []\n",
    "\n",
    "    # os dados estão apenas no segundo element td\n",
    "    table_data = tabelas[5].select('tr')[0].select('td')[1]\n",
    "    suffix_element = table_data.select('font')  # título da segunda subtabela\n",
    "    table_elements = table_data.select('table')\n",
    "\n",
    "    # 1a sub tabela\n",
    "    suffix = ''\n",
    "    for table_element in table_elements:\n",
    "        columns = []\n",
    "        data = []\n",
    "        for idx, table_row in enumerate(table_element.select('tr')):\n",
    "            if idx == 0:\n",
    "                suffix = table_row.get_text(strip=True)\n",
    "                continue\n",
    "            table_data = table_row.select('td')\n",
    "            unidade = ''\n",
    "            if len(table_data) == 3:\n",
    "                column, value, unidade = table_data\n",
    "                unidade = unidade.get_text(strip=True).replace(' ', '_')\n",
    "            else:\n",
    "                column, value = table_data\n",
    "            column = f\"{suffix}_{column.get_text(strip=True)}\".replace(' ', '_').replace(':', '')\n",
    "            columns.append(unidecode(f\"{column}_{unidade}\").upper())\n",
    "            data.append(value.get_text(strip=True))\n",
    "            \n",
    "        dataframes_list.append(pd.DataFrame([data], columns=columns))\n",
    "    \n",
    "    return dataframes_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal\n",
    "\n",
    "A célula a seguir contém o loop principal.\n",
    "\n",
    "Nesta versão, o programa primeiramente requere `chunk_size` páginas, e então extrai e salva os dados, \n",
    "para só então fazer outras novas `chunk_size` requisições, logo depois de pausar a execução por `sleep_time` segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando arquivo de links\n",
    "FILENAME = 'Atlantico_Sul_N_NE.txt'\n",
    "LINKS = []\n",
    "with open(FILENAME, encoding='utf-8') as file_:\n",
    "    LINKS = file_.read().split('\\n')\n",
    "\n",
    "######\n",
    "# NOTE: O PROGRAMA PAROU EM ALGUM PONTO? AJUSTE AQUI!\n",
    "######\n",
    "\n",
    "start = 2540\n",
    "end = len(links)  # len(links)  # use len(links) para ir até o último link\n",
    "\n",
    "######\n",
    "\n",
    "chunk_size = 10  # A cada `chunk_size` requisições a execução será\n",
    "sleep_time = 1   # pausada por `sleep_time` segundos\n",
    "\n",
    "error = None\n",
    "\n",
    "async with httpx.AsyncClient() as client:\n",
    "    for i in tqdm.notebook.tqdm(range(start, end, chunk_size)):\n",
    "        chunk = LINKS[i:i + chunk_size]\n",
    "\n",
    "        chunk_responses = await fetch_links(client, chunk)\n",
    "\n",
    "        ocorrencias = []\n",
    "\n",
    "        # os dados serão extraídos e salvos a cada pedaço\n",
    "        # antes de realizar o próximo pedaço de requisições\n",
    "        for response_idx, response in enumerate(chunk_responses):\n",
    "\n",
    "            ocorrencia = []\n",
    "            soup = BeautifulSoup(response.text, features='lxml')\n",
    "            # todas as tabelas (elementos dentro de elementos de id=\"tabs-n\")\n",
    "            tabelas = soup.select('#newsContent')\n",
    "\n",
    "            try:\n",
    "                ocorrencia.append(tabela_1(tabelas))\n",
    "                ocorrencia.extend(tabela_2(tabelas))\n",
    "                ocorrencia.extend(tabela_3(tabelas))\n",
    "                ocorrencia.extend(tabela_4(tabelas))\n",
    "                ocorrencia.extend(tabela_5(tabelas))\n",
    "                ocorrencia.extend(tabela_6(tabelas))\n",
    "            except CustomError as err:\n",
    "                if err.message == 'pular':\n",
    "                    continue\n",
    "\n",
    "            ocorrencias.append(pd.concat(ocorrencia, axis=1))\n",
    "\n",
    "        # salva a cada chunk_size páginas (para liberar a memória)\n",
    "        fname = f\"samples/sample_{i}_to_{i+chunk_size}.parquet\"\n",
    "        # remove colunas duplicadas\n",
    "        dfs = [df.loc[:, ~df.columns.duplicated()].reset_index() for df in ocorrencias]\n",
    "        # merge o resultado\n",
    "        amostra = pd.concat(dfs).iloc[:, 1:]\n",
    "        amostra.to_parquet(fname, engine=\"pyarrow\", index=False)\n",
    "\n",
    "        time.sleep(sleep_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "amostra = None\n",
    "for parquet_file in Path().glob('samples/*.parquet'):\n",
    "    if amostra is None:\n",
    "        amostra = pd.read_parquet(parquet_file)\n",
    "        continue\n",
    "    amostra = pd.concat([amostra, pd.read_parquet(parquet_file)])\n",
    "\n",
    "# resetando os índices (eles ficam repetidos)\n",
    "amostra = amostra.reset_index().iloc[:, 1:]\n",
    "amostra.head(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
